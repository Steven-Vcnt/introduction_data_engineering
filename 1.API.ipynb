{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and Key Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from io import StringIO\n",
    "from zipfile import ZipFile\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_API_Key="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyword='societe'\n",
    "url = 'https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={searchKeyword}&apikey={apiKey}'.format(apiKey=AV_API_Key, searchKeyword=search_keyword)\n",
    "r_search = requests.get(url)\n",
    "js_search = r_search.json()\n",
    "df_search=pd.DataFrame(js_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_time_series=pd.DataFrame()\n",
    "ticker_metadata=pd.DataFrame()\n",
    "ticker_list=['GLE','ORA']\n",
    "for ticker in ticker_list :\n",
    "    r_stock = requests.get('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))\n",
    "    js_stock = r_stock.json()\n",
    "    # Ticker time series to DataFrame\n",
    "    df_time_series=pd.DataFrame(js_stock['Time Series (Daily)'])\n",
    "    df_time_series['Ticker']=ticker\n",
    "    ticker_time_series=pd.concat([ticker_time_series, df_time_series])\n",
    "    # Ticker metadata to DataFrame\n",
    "    df_metadata=pd.json_normalize(js_stock['Meta Data'])\n",
    "    df_metadata['Ticker']=ticker\n",
    "    ticker_metadata=pd.concat([ticker_metadata, df_metadata])\n",
    "display(ticker_time_series.head())\n",
    "display(ticker_metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r_listing_status = requests.get('https://www.alphavantage.co/query?function=LISTING_STATUS&apikey={apiKey}'.format(apiKey=AV_API_Key))\n",
    "df_listing_status=pd.read_csv(StringIO(r_listing_status.content.decode()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_status['exchange'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_ticker=df_listing_status[df_listing_status['exchange']=='PARIS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_status.to_csv(os.path.join(base_path, 'ticker_listing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_information=pd.DataFrame()\n",
    "for ticker in nasdaq_ticker['symbol']:\n",
    "    r_overview = requests.get('https://www.alphavantage.co/query?function=OVERVIEW&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))\n",
    "    js_overview=r_overview.json()\n",
    "    df_overview=pd.json_normalize(js_overview)\n",
    "    ticker_information=pd.concat([ticker_information, df_overview])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Company ovierview from symbol/ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_information=pd.DataFrame()\n",
    "ticker_list=['GOOGL','AAPL', 'META', 'AMZN', 'MSFT']\n",
    "for ticker in ticker_list :\n",
    "    r_overview = requests.get('https://www.alphavantage.co/query?function=OVERVIEW&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))\n",
    "    js_overview=r_overview.json()\n",
    "    df_overview=pd.json_normalize(js_overview)\n",
    "    ticker_information=pd.concat([ticker_information, df_overview])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_information['Symbol'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the error here\n",
    "ticker_stock=pd.DataFrame()\n",
    "ticker_list=['GOOGL','AAPL', 'META', 'AMZN', 'MSFT']\n",
    "for ticker in ticker_list :\n",
    "    r_stock = requests.get('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))\n",
    "    js_stock = r_stock.json()\n",
    "    df_stock=pd.json_normalize(js_stock)\n",
    "    ticker_stock=pd.concat([ticker_stock, js_stock])\n",
    "display(ticker_stock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_time_series=pd.DataFrame()\n",
    "ticker_metadata=pd.DataFrame()\n",
    "ticker_list=['GOOGL','AAPL', 'META', 'AMZN', 'MSFT']\n",
    "for ticker in ticker_list :\n",
    "    r_stock = requests.get('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol={ticker}&apikey={apiKey}'.format(apiKey=AV_API_Key, ticker=ticker))\n",
    "    js_stock = r_stock.json()\n",
    "    # Ticker time series to DataFrame\n",
    "    df_time_series=pd.DataFrame(js_stock['Time Series (Daily)'])\n",
    "    df_time_series['Ticker']=ticker\n",
    "    ticker_time_series=pd.concat([ticker_time_series, df_time_series])\n",
    "    # Ticker metadata to DataFrame\n",
    "    df_metadata=pd.json_normalize(js_stock['Meta Data'])\n",
    "    df_metadata['Ticker']=ticker\n",
    "    ticker_metadata=pd.concat([ticker_metadata, df_metadata])\n",
    "display(ticker_time_series.head())\n",
    "display(ticker_metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_time_series=ticker_time_series.reset_index().rename({'index':'Value_type'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_time_series=ticker_time_series.reset_index().rename({'index':'Value_type'}, axis=1)\n",
    "lg_ticker_ts = ticker_time_series.melt(id_vars=['Ticker','Value_type'])*\n",
    "display(lg_ticker_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account='datalakeintrocloud'\n",
    "sas_key=''\n",
    "spark.conf.set(\"fs.azure.account.auth.type.{str_account}.dfs.core.windows.net\".format(str_account=storage_account), \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.{str_account}.dfs.core.windows.net\".format(str_account=storage_account), \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.{str_account}.dfs.core.windows.net\".format(str_account=storage_account), sas_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp_dvf_file=spark.read.csv('abfs://containerintro@introductiondatastorage.dfs.core.windows.net/immodata/valeursfoncieres-2021.txt',sep=\"|\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_sp_dvf = sp_dvf_file.select([F.col(col).alias(col.replace(' ', '_')) for col in sp_dvf_file.columns])\n",
    "dot_sp_dvf = renamed_sp_dvf.withColumn('Valeur_fonciere', regexp_replace('Valeur_fonciere', ',', '.').cast(\"int\"))\n",
    "dot_sp_dvf = dot_sp_dvf.withColumn('Surface_reelle_bati', dot_sp_dvf['Surface_reelle_bati'].cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_sp_dvf.write.format(\"delta\").save(\"/FileStore/tables/dvf_2021\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adls_read_file(adls, container, SASKey, csvpath, separator):\n",
    "    '''\n",
    "    Read a flat file stored in an Azure Data Lake Storage. You need the following arguments to make it work:\n",
    "    - adls : Azure Data Lake Storage name\n",
    "    - container : Container name of your Data Lake Storage\n",
    "    - SASKey : Azure Data Lake Storage Shared Acess Signature key\n",
    "    - csvpath : Path of your file\n",
    "    - separator : Separator of your flat file\n",
    "    '''\n",
    "    spark.conf.set(\"fs.azure.account.auth.type.{adls}.dfs.core.windows.net\".format(adls=adls), \"SAS\")\n",
    "    spark.conf.set(\"fs.azure.sas.token.provider.type.{adls}.dfs.core.windows.net\".format(adls=adls), \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "    spark.conf.set(\"fs.azure.sas.fixed.token.{adls}.dfs.core.windows.net\".format(adls=adls), str(SASKey))\n",
    "    sp_dvf_file=spark.read.csv('abfs://{container}@{adls}.dfs.core.windows.net/{csvpath}'.format(adls=adls, container=container, csvpath=csvpath),sep=separator, header=True)\n",
    "    return sp_dvf_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('https://adlsintrosteven.blob.core.windows.net/adlsintro/list_NASDAQ_ticker.csv?sp=r&st=2024-01-02T14:02:47Z&se=2024-01-02T22:02:47Z&spr=https&sv=2022-11-02&sr=b&sig=EpFH2O3crKVg3iQKdPr7kngh0rNEc07NSfefn9TQ6Lc%3D', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(\"https://adlsintrosteven.blob.core.windows.net/adlsintro/list_NASDAQ_ticker4.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-02-14T01:51:44Z&st=2023-12-28T11:01:44Z&spr=https&sig=xYt%2FGViep0o2%2F4CWdU0xP8eneenl%2BLXjboPSSZ0RPao%3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "# Define your ADLS Gen2 credentials and file details\n",
    "account_name = 'adlsintrosteven'\n",
    "account_key = \n",
    "file_system_name = 'adlsintro'\n",
    "file_path = 'ticker_listing.csv'\n",
    "\n",
    "# Create a DataLakeServiceClient\n",
    "service_client = DataLakeServiceClient(account_url=f\"https://{account_name}.dfs.core.windows.net\", credential=account_key)\n",
    "\n",
    "# Get a file system client\n",
    "file_system_client = service_client.get_file_system_client(file_system=file_system_name)\n",
    "\n",
    "# Get a file client\n",
    "file_client = file_system_client.get_file_client(file_path)\n",
    "\n",
    "# Read the file content\n",
    "download = file_client.download_file()\n",
    "downloaded_bytes = download.readall()\n",
    "\n",
    "# Convert bytes to a StringIO object\n",
    "file_content = StringIO(downloaded_bytes.decode('utf-8'))\n",
    "\n",
    "# Read the file content into a pandas DataFrame\n",
    "df = pd.read_csv(file_content, sep=';')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
